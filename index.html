<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://safolab-wisc.github.io/llm-armor/" target="_blank">Zhengyue Zhao</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://safolab-wisc.github.io/llm-armor/" target="_blank">Yingzi Ma</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://safolab-wisc.github.io/llm-armor/" target="_blank">Somesh Jha</a><sup>1</sup>,</span>
                    <span class="author-block">
                    <a href="https://safolab-wisc.github.io/llm-armor/" target="_blank">Marco Pavone</a><sup>2,3</sup>,</span>
                    <span class="author-block">
                    <a href="https://safolab-wisc.github.io/llm-armor/" target="_blank">Chaowei Xiao</a><sup>1</sup></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Wisconsin-Madison<br><sup>2</sup>Stanford University<br><sup>3</sup>NVIDIA<br></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                  <!-- ArXiv abstract Link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2507.11500" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/SaFoLab-WISC/armor" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Arxiv PDF link -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 800px; margin: 0 auto;">
      <img src="static/images/overview.svg" style="display: block; margin: 0 auto;" alt="Overview" width="100%"/>
        <h2 class="content has-text-justified">
          <b>Figure 1. Overview of ARMOR:</b> Reasoning-based safety-aligned LLMs encounter "hidden intent injection" when facing adaptive jailbreak attacks, resulting in a misaligned output. In contrast, ARMOR extracts the core intent of the instruction with a jailbreak strategy analysis, along with a policy-based safety analysis, demonstrating robustness to adaptive jailbreak attacks. 
      </div>
    </div>
  </section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop" style="max-width: 800px; margin: 0 auto;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) have demonstrated remarkable generative capabilities. However, their susceptibility to misuse has raised significant safety concerns. While post-training safety alignment methods have been widely adopted, LLMs remain vulnerable to malicious instructions that can bypass safety constraints. Recent efforts have introduced inference-time safety reasoning (system-2 alignment), where LLMs conduct a reasoning process to perform safety verification before final response. We show, however, that these checks are driven by ad-hoc reasoning that diverges from the structured human process, where they first discern a user's true intent, then evaluate the associated risk based on the true intent. Consequently, these defenses remain vulnerable to sophisticated jailbreak prompts that cloak harmful goals in seemingly benign language. To build secure and safe LLMs, we propose a reasoning-based safety alignment framework, ARMOR, that replaces the ad-hoc chains of thought reasoning process with human-aligned, structured one. At inference, ARMOR (1) detects likely jailbreak strategies, (2) extracts the user's core intent while discarding deceptive instructions, and (3) applies a policy-grounded safety analysis to the purified request. ARMOR is evaluated on adaptive jailbreak attacks and multiple safety benchmarks, and a test-time scaling is conducted to further improve its performance. Results demonstrate that ARMOR significantly enhances the robustness against state-of-the-art adaptive jailbreak attacks and outperforms recent reasoning-based aligned models across various safety benchmarks.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 800px; margin: 0 auto;">
      <h2 class="title">Method</h2>
      <div style="margin-bottom: 2rem;">
        <h2 class="subtitle has-text-justified">
          Details of construction of Meticulous Reasoning Steps, which contains the <b>strategy analysis</b> step, <b>intent analysis</b> step, and <b>safety analysis</b> step followed with the <b>final answer</b>. Each step is build with the ground truth data including the intent (original prompt), the jailbreak prompt, the specified strategy and the related safety policy.
        </h2>
      </div>
      <img src="static/images/build.svg" style="display: block; margin: 2rem auto;" alt="Construction" width="85%"/>
      <h2 class="subtitle has-text-centered">
          <b>Figure 2.</b> Construction of Meticulous Reasoning.
        </h2>
      <div style="margin: 2rem 0;">
        <p class="subtitle has-text-justified">
          The framework of ARMOR consists of the following steps: (1) Construct the Meticulous Reasoning steps with jailbreak prompts, their coordinate ground truth (GT) jailbreak strategy and intent, and the safety policy; (2) Format the reasoning steps with inputs involving the user's prompts and the system prompt consists of a dynamic strategy library and the safety policy; (3) Train the base model to get the ARMOR model; (4) Conduct inference of ARMOR with a custom strategy library and the safety policy; (5) Conduct test-time scaling with the DPO model and PRM trained on preference data generated from grounded tree sampling.
        </p>
      </div>
      
      <img src="static/images/method.svg" style="display: block; margin: 2rem auto;" alt="Framework" width="100%"/>
      <p class="subtitle has-text-centered">
          <b>Figure 3.</b> Framework of ARMOR.
        </p>
    </div>
  </div>
</section>

 <section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 800px; margin: 0 auto;">
      <h2 class="title">Result</h2>
      <img src="static/images/Results.png" style="display: block; margin: 0 auto;" alt="Results" width="90%"/>
      </div>
    </div>
  </section>


  <section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 800px; margin: 0 auto;">
      <h2 class="title">Examples</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/case_jailbreak.svg"  style="display: block; margin: 0 auto;" alt="Results" width="80%"/>
        <h2 class="subtitle has-text-centered">
          An example of jailbreak prompt.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/case_harmful.svg"  style="display: block; margin: 0 auto;" alt="Results" width="80%"/>
        <h2 class="subtitle has-text-centered">
          An example of direct harmful prompt.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/case_benign.svg"  style="display: block; margin: 0 auto;" alt="Results" width="80%"/>
        <h2 class="subtitle has-text-centered">
          An example of benign prompt.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Example</h2>
      <img src="static/images/case_jailbreak.svg" style="display: block; margin: 0 auto;" alt="Example" width="60%"/>
        <h2 class="content has-text-justified">
          An example of jailbreak prompt.
        </h2>
      </div>
      <img src="static/images/case_harmful.svg" style="display: block; margin: 0 auto;" alt="Example" width="50%"/>
        <h2 class="content has-text-justified">
          An example of direct harmful prompt.
        </h2>
      </div>
      <img src="static/images/case_benign.svg" style="display: block; margin: 0 auto;" alt="Example" width="50%"/>
        <h2 class="content has-text-justified">
          An example of benign prompt.
        </h2>
      </div>
    </div>
  </section> -->









  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <!-- You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. -->
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
</body>
</html>
